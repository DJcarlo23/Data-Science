{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import math\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing\n",
    "from statsmodels.tsa.api import Holt\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from pandas.plotting import lag_plot\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from math import sqrt\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/sales_train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gona change 'date' feature to other format. It's good practice beacause our new format will be used by many functions in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change format of date feature\n",
    "df['date'] = pd.to_datetime(df.date, format = '%d.%m.%Y')\n",
    "df.head().style.set_properties(subset=['date'], **{\n",
    "    'background-color': 'dodgerblue'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = df.groupby('date')['item_cnt_day'].sum()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=plot.index, y=plot.values)\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Number of sold products every day',\n",
    "    yaxis_title='Number of products',\n",
    "    xaxis_title='Month'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = df.groupby('date', as_index=False)['item_cnt_day'].sum()\n",
    "resampled_plot = plot[['date', 'item_cnt_day']].resample('7D', on='date').sum().reset_index(drop=False)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=resampled_plot.date, y=resampled_plot.item_cnt_day)\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Number of sold products every week',\n",
    "    yaxis_title='Number of products',\n",
    "    xaxis_title='Month'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = df.groupby('date_block_num')['item_cnt_day'].sum()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=plot.index, y=plot.values)\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Number of sold products every month',\n",
    "    yaxis_title='Number of products',\n",
    "    xaxis_title='Month'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no necessity to look at the daily data. Considering weekly data seems to be sufficient as well. We gona create new dataframe with number of sold_products ordered by date and we will downsample it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_prod = pd.DataFrame(df.groupby('date', as_index=False)['item_cnt_day'].sum()).rename(columns={'item_cnt_day': 'sold_products'})\n",
    "df_downsampled = df_num_prod[[\n",
    "    'date',\n",
    "    'sold_products'\n",
    "]].resample('7D', on='date').sum().reset_index(drop=False)\n",
    "df_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for i in range(2):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_downsampled.date, y=df_downsampled['sold_products'].shift(-i), name='lag ' + str(i))\n",
    "    )\n",
    "    \n",
    "fig.update_xaxes(title_text='Date')\n",
    "fig.update_yaxes(title_text='Sold Products')\n",
    "fig.update_layout(title='Lag plot')\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this function plots the temperature data (t) on the x-axis\n",
    "autocorrelation_plot(df_downsampled['sold_products'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(df_downsampled['sold_products'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(df_downsampled['sold_products'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this function plots the temperature data (t) on the x-axis against the temperature on the previous data \n",
    "# (t-1) on the y-axis\n",
    "lag_plot(df_downsampled['sold_products'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data have a trend, the autocorrelations for small lags tend to be large and positive because observations nearby in time are also nearby in size. So the ACF of trended time series tend to have positive values that slowly decrease as the lags increase.\n",
    "When data are seasonal, the autocorrelations will be larger for the seasonal lags (at multiples of the seasonal frequency) than for other lags.\n",
    "When data are both trended and seasonal, you see a combination of these effects.\n",
    "In our case we can see strong trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For white noise series, we expect each autocorrelation to be cose to zero. Of course, they will not be exactly equal to zero as there is some random variation. For a white noise series, we expect 95% of the spikes in the ACF to lie within plus-minus 2/sqrt(T) where T is the lenght of the time series. In our case autocorrelation coefficients are outside of the range that is why our data are not white noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Simple forecasting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, the forecasts of all future values are equal to the average\n",
    "rang = 20\n",
    "mean = df_downsampled['sold_products'].mean()\n",
    "means = []\n",
    "for i in range(rang):\n",
    "    means.append(mean)\n",
    "    \n",
    "new = pd.date_range(df_downsampled.date.iloc[-1], periods=rang, freq='W')\n",
    "new_df = pd.DataFrame({'date': new[1:], 'sold_products': mean})\n",
    "copy_df = df_downsampled.copy()\n",
    "\n",
    "to_plot = pd.concat([copy_df, new_df])\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=to_plot.date, y=to_plot.sold_products, name='basic')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=new_df.date, y=new_df.sold_products, name='predicted', mode='lines')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For naïve forecasts, we simply set all forecasts to be the value of the last observation\n",
    "rang = 20\n",
    "mean = df_downsampled['sold_products'].iloc[-1]\n",
    "means = []\n",
    "for i in range(rang):\n",
    "    means.append(mean)\n",
    "    \n",
    "new = pd.date_range(df_downsampled.date.iloc[-1], periods=rang, freq='W')\n",
    "new_df = pd.DataFrame({'date': new[1:], 'sold_products': mean})\n",
    "copy_df = df_downsampled.copy()\n",
    "\n",
    "to_plot = pd.concat([copy_df, new_df])\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=to_plot.date, y=to_plot.sold_products, name='basic')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=new_df.date, y=new_df.sold_products, name='predicted', mode='lines')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp = seasonal_decompose(df_downsampled.sold_products, freq=10, model='additive', extrapolate_trend='freq')\n",
    "df_downsampled['sold_products_trend'] = decomp.trend\n",
    "df_downsampled['sold_products_seasonal'] = decomp.seasonal\n",
    "df_downsampled['sold_products_residual'] = decomp.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(cols=1, rows=4, subplot_titles=(\n",
    "    'Basic',\n",
    "    'Trend',\n",
    "    'Seasonality',\n",
    "    'Residual'\n",
    "))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_trend),\n",
    "    row=2,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_seasonal),\n",
    "    row=3,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_residual),\n",
    "    row=4,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800, title_text='Eecomposition of Sold Products', showlegend=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot time series and check for trends or seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products, name='basic')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products.rolling(10).mean(), name='rolling mean')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products.rolling(10).std(), name='rolling std')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADF test is used to determine the presense of unit root in the series, and hence helps in understanding if the series is stationary or not. The null and alternate hypothesis of this test are:\n",
    "Null Hypothesis: The series has a unit root\n",
    "Alternate Hypothesis: The series has no unit root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(timeseries):\n",
    "    print('Results of Diskey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KPSS is another test for checking the stationarity of a time series. The null and alternate hypothesis for the KPSS test are opposite that of the ADF test:\n",
    "Null Hypothesis: The process is trend stationary\n",
    "Alternate Hypothesis: The series has a unit root (series is not stationary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpss_test(timeseries):\n",
    "    print('Results of KPSS Test:')\n",
    "    kpsstest = kpss(timeseries, regression='c', nlags='auto')\n",
    "    kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic', 'p-value', 'Lags Used'])\n",
    "    for key, value in kpsstest[3].items():\n",
    "        kpss_output['Critical Value (%s)'%key] = value\n",
    "    print(kpss_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_test(df_downsampled.sold_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the test statistic is less than the critical value, we can reject the null hypothesis (aka the series is stationary). When the test statistic is greater than the critical value, we fail to reject the null hypothesis (which means the series is not stationary). In out case, the test statistic > critical value, which implies that the series is not stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpss_test(df_downsampled.sold_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the test statistic is greater than the critical value, we reject the null hypothesis (series is not stationary). If the test statistic is less than the critical value, it fail to reject the null hypothesis (series is stationary). In our case, the test statistic > critical value, which again implies that the series is not stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both tests we see that our data are not stationary. That is why we will use differencing to make it stationary for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Order Differencing\n",
    "ts_diff = np.diff(df_downsampled.sold_products)\n",
    "df_downsampled['sold_products_diff_1'] = np.append([0], ts_diff)\n",
    "\n",
    "# Second Order Differencing\n",
    "ts_diff = np.diff(df_downsampled.sold_products_diff_1)\n",
    "df_downsampled['sold_products_diff_2'] = np.append([0], ts_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(cols=1, rows=2, subplot_titles=(\n",
    "    'sold_products_diff_1',\n",
    "    'sold_products_diff_2'\n",
    "))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_1, legendgroup='basic', line=dict(color = 'blue'), name='basic'),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_1.rolling(10).mean(), legendgroup='mean', line=dict(color = 'orange'), name='rolling mean'),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_1.rolling(10).std(), legendgroup='std', line=dict(color = 'red'), name='rolling std'),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "fig.update_yaxes(title_text='Number of Sold Products', row=1, col=1)\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_2, legendgroup='basic', line=dict(color = 'blue'), showlegend=False),\n",
    "    row=2,\n",
    "    col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_2.rolling(10).mean(), legendgroup='mean', line=dict(color = 'orange'), showlegend=False),\n",
    "    row=2,\n",
    "    col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_downsampled.date, y=df_downsampled.sold_products_diff_2.rolling(10).std(), legendgroup='std', line=dict(color = 'red'), showlegend=False),\n",
    "    row=2,\n",
    "    col=1\n",
    ")\n",
    "fig.update_yaxes(title_text='Number of Sold Products', row=2, col=1)\n",
    "\n",
    "fig.update_layout(showlegend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_test(df_downsampled.sold_products_diff_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After differencing we can see that, the ADF test statistic < critical value, which implies that the series is now stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpss_test(df_downsampled.sold_products_diff_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After differencing we can see that, the KPSS test statistic < critical value, which implies that the series is now stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(df_downsampled.sold_products_diff_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(df_downsampled.sold_products_diff_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X = df_downsampled.copy()\n",
    "X = X.drop(['date'], axis=1)\n",
    "X = X.sold_products_diff_1.values\n",
    "train, test = X[1:len(X)-7], X[len(X)-7:]\n",
    "\n",
    "# Train autoregression\n",
    "model = AutoReg(train, lags=2)\n",
    "model_fit = model.fit()\n",
    "print('Coefficients %s' % model_fit.params)\n",
    "print()\n",
    "\n",
    "# Make predictions\n",
    "predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    print('predicted=%f, expected=%f' % (predictions[i], test[i]))\n",
    "\n",
    "print()\n",
    "    \n",
    "rmse = sqrt(mean_squared_error(test, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# Plot results\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(7), y=test, name='test')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(7), y=predictions, name='predictions')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(df_downsampled.set_index('date').sold_products, order=(5, 1, 0))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_downsampled.set_index('date').sold_products\n",
    "size = int(len(X) * 0.8)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "\n",
    "# walk-forward validation\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(history, order=(2,1,0))\n",
    "    model_fit = model.fit()\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)\n",
    "    print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "\n",
    "print()\n",
    "# Evaluate forecast\n",
    "rmse = sqrt(mean_squared_error(test, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# Plot forecast against actual outcomes\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(30), y=test, name='test')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(30), y=predictions, name='predictions')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_arima_model(X, arima_order):\n",
    "    # prepare training dataset\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    history = [x for x in train]\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order=arima_order)\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(test[t])\n",
    "    # calculate out of sample error\n",
    "    error = mean_squared_error(test, predictions)\n",
    "    return sqrt(error)\n",
    "\n",
    "# GriSearchCV for ARIMA\n",
    "def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "    dataset = dataset.astype('float32')\n",
    "    best_score, best_cfg = float('inf'), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p, d, q)\n",
    "                try:\n",
    "                    rmse = evaluate_arima_model(dataset, order)\n",
    "                    if rmse < best_score:\n",
    "                        best_score, best_cfg = rmse, order\n",
    "                    print('ARIMA%s RMSE=%.3f' % (order, rmse))\n",
    "                except:\n",
    "                    continue\n",
    "    print('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Exponential Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest of the exponentially smoothing methods is naturally called simple exponencial smoothing (SES). This method is suitable for forecasting data with no clear trend or seasonal pattern. Here we gona use it only to show how it is works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_downsampled.set_index('date').sold_products\n",
    "fit1 = SimpleExpSmoothing(X, initialization_method='heuristic').fit(smoothing_level=0.2, optimized=False)\n",
    "fcast1 = fit1.forecast(20)\n",
    "\n",
    "\n",
    "fit2 = SimpleExpSmoothing(X, initialization_method='heuristic').fit(smoothing_level=0.4, optimized=False)\n",
    "fcast2 = fit1.forecast(20)\n",
    "\n",
    "fit3 = SimpleExpSmoothing(X, initialization_method='heuristic').fit(smoothing_level=0.6, optimized=False)\n",
    "fcast3 = fit1.forecast(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X.index, y=X.values, name='test')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fit1.fittedvalues.index, y=fit1.fittedvalues.values, legendgroup='0.2', name='SES 0.2', line=dict(color = 'red'))\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fcast1.index, y=fcast1.values, legendgroup='0.2', showlegend=False, line=dict(color = 'red'))\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fit2.fittedvalues.index, y=fit2.fittedvalues.values, legendgroup='0.4', name='SES 0.4', line=dict(color = 'yellow'))\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fcast2.index, y=fcast2.values, legendgroup='0.4', showlegend=False, line=dict(color = 'yellow'))\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fit3.fittedvalues.index, y=fit3.fittedvalues.values, legendgroup='0.6', name='SES 0.6', line=dict(color='brown'))\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fcast3.index, y=fcast3.values, legendgroup='0.6', showlegend=False, line=dict(color = 'brown'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_downsampled.set_index('date').sold_products\n",
    "size = int(len(X) * 0.8)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "\n",
    "# walk-forward validation\n",
    "for t in range(len(test)):\n",
    "    model = SimpleExpSmoothing(history, initialization_method='heuristic').fit(smoothing_level=0.2, optimized=False)\n",
    "#     model_fit = model.fit()\n",
    "    output = model.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)\n",
    "    print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "\n",
    "print()\n",
    "# Evaluate forecast\n",
    "rmse = sqrt(mean_squared_error(test, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# Plot forecast against actual outcomes\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(30), y=test, name='test')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(30), y=predictions, name='predictions')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holt's Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_downsampled.set_index('date').sold_products\n",
    "fit1 = Holt(X, initialization_method='estimated').fit(smoothing_level=0.8, smoothing_trend=0.2, optimized=False)\n",
    "fcast1 = fit1.forecast(10)\n",
    "\n",
    "fit2 = Holt(X, initialization_method='estimated').fit(smoothing_level=0.6, smoothing_trend=0.4, optimized=False)\n",
    "fcast2 = fit2.forecast(10)\n",
    "\n",
    "fit3 = Holt(X, initialization_method='estimated').fit(smoothing_level=0.4, smoothing_trend=0.6, optimized=False)\n",
    "fcast3 = fit3.forecast(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X.index, y=X.values, name='test')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fit1.fittedvalues.index, y=fit1.fittedvalues.values, legendgroup='HES 0.8 0.2', line=dict(color = 'red'), name='HES 0.8 0.2')\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fcast1.index, y=fcast1.values, mode='lines', legendgroup='HES 0.8 0.2', line=dict(color='red'), showlegend=False)\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fit2.fittedvalues.index, y=fit2.fittedvalues.values, legendgroup='HES 0.6 0.4', line=dict(color = 'yellow'), name='HES 0.6 0.4')\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fcast2.index, y=fcast2.values, mode='lines', legendgroup='HES 0.6 0.4', line=dict(color='yellow'), showlegend=False)\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fit3.fittedvalues.index, y=fit3.fittedvalues.values, legendgroup='HES 0.4 0.6', line=dict(color = 'brown'), name='HES 0.4 0.6')\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fcast3.index, y=fcast3.values, mode='lines', legendgroup='HES 0.4 0.6', line=dict(color='brown'), showlegend=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_downsampled.set_index('date').sold_products\n",
    "size = int(len(X) * 0.8)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "\n",
    "# walk-forward validation\n",
    "for t in range(len(test)):\n",
    "    model = Holt(history, initialization_method='estimated').fit(smoothing_level=0.8, smoothing_trend=0.2, optimized=False)\n",
    "    output = model.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)\n",
    "    print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "\n",
    "print()\n",
    "# Evaluate forecast\n",
    "rmse = sqrt(mean_squared_error(test, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# Plot forecast against actual outcomes\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(30), y=test, name='test')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(30), y=predictions, name='predictions')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated = model_fit.simulate(anchor='end', nsimulations=7, repetitions=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(simulated)):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=np.arange(20), y=simulated[i], line=dict(color = 'gray'), showlegend=False, opacity=0.2)\n",
    "    )\n",
    "    \n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(20), y=test, line=dict(color = 'red'))\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holt's Winters Seasonal Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_downsampled.set_index('date').sold_products\n",
    "\n",
    "fit1 = ExponentialSmoothing(X, seasonal_periods=12, trend='add', seasonal='add', use_boxcox=True, initialization_method='estimated').fit()\n",
    "fcast1 = fit1.forecast(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X.index, y=X.values, name='data')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fit1.fittedvalues.index, y=fit1.fittedvalues.values, legendgroup='add add', line=dict(color = 'red'), name='predicted')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fcast1.index, y=fcast1.values, legendgroup='add add', line=dict(color = 'red'), showlegend=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = df_downsampled.set_index('date').sold_products\n",
    "size = int(len(X) * 0.8)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "\n",
    "# walk-forward validation\n",
    "for t in range(len(test)):\n",
    "    model = ExponentialSmoothing(history, seasonal_periods=7, trend='mul', seasonal='mul', use_boxcox=True, initialization_method='estimated')\n",
    "    model_fit = model.fit()\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)\n",
    "    print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "\n",
    "print()\n",
    "# Evaluate forecast\n",
    "rmse = sqrt(mean_squared_error(test, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# Plot forecast against actual outcomes\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(30), y=test, name='test')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(30), y=predictions, name='predictions')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated = model_fit.simulate(anchor='end', nsimulations=7, repetitions=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(simulated)):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=np.arange(20), y=simulated[i], line=dict(color = 'gray'), showlegend=False, opacity=0.2)\n",
    "    )\n",
    "    \n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(20), y=test, line=dict(color = 'red'))\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
