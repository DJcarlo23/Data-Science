{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "# Removing stop words like 'the', 'a', 'an', 'in'\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Return the ngrams generated from a sequence of items, as an iterator\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Convert a collection of text documents to a matrix of token counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from  sklearn.metrics  import accuracy_score, confusion_matrix\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.naive_bayes import MultinomialNB # good one\n",
    "from sklearn.linear_model import LogisticRegression # best\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = pd.read_csv('D:/Python/Anaconda/Kaggle/Working/Natural Language Processing with Disaster Tweets/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('D:/Python/Anaconda/Kaggle/Working/Natural Language Processing with Disaster Tweets/nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\n",
    "print('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "to_plot = tweet['target'].replace({0: 'No disaster', 1: 'Disaster'}).value_counts()\n",
    "labels = to_plot.index\n",
    "values = to_plot.values\n",
    "\n",
    "fig.add_trace(go.Pie(\n",
    "    labels = labels,\n",
    "    values = values,\n",
    "    textinfo='label+percent'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Target balance',\n",
    "    template='plotly_dark'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Number of characters in tweets</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Disaster tweets', 'No disaster tweets'))\n",
    "\n",
    "tweet_len_1 = tweet[tweet['target']==1]['text'].str.len()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(x = tweet_len_1, name='Disaster tweets'),\n",
    "    row=1,\n",
    "    col=1\n",
    "    )\n",
    "\n",
    "\n",
    "tweet_len_0 = tweet[tweet['target']==0]['text'].str.len()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(x = tweet_len_0, name='No disaster tweets'),\n",
    "    row=1,\n",
    "    col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    title_text='Characters in tweets',\n",
    "    template='plotly_dark'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Number of words in tweets</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Disaster tweets', 'No disaster tweets'))\n",
    "\n",
    "tweet_len_1 = tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(x = tweet_len_1, name='Disaster tweets'),\n",
    "    row=1,\n",
    "    col=1\n",
    "    )\n",
    "\n",
    "\n",
    "tweet_len_0 = tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(x = tweet_len_0, name='No disaster tweets'),\n",
    "    row=1,\n",
    "    col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    title_text='Characters in words',\n",
    "    template='plotly_dark'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Average word length in a tweets</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dis = tweet[tweet['target']==1]['text'].str.split().apply(lambda x: [len(i) for i in x])\n",
    "word_ndis = tweet[tweet['target']==0]['text'].str.split().apply(lambda x: [len(i) for i in x])\n",
    "\n",
    "fig = ff.create_distplot(\n",
    "    [word_dis.map(lambda x: np.mean(x)), word_ndis.map(lambda x: np.mean(x))],\n",
    "    ['Disaster', 'No disaster'],\n",
    "    bin_size=0.2,\n",
    "    colors=['#ff5050', '#0066ff']\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Average word length in each tweet',\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Common stopwords in tweets</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_list(target):\n",
    "    words = []\n",
    "\n",
    "    for x in tweet[tweet['target']==target]['text'].str.split():\n",
    "        for i in x:\n",
    "            words.append(i)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list_0 = words_list(0)\n",
    "dic = defaultdict(int)\n",
    "\n",
    "for word in words_list_0:\n",
    "    if word in stop:\n",
    "        dic[word] += 1\n",
    "        \n",
    "top=sorted(dic.items(), key=lambda x:x[1], reverse=True)[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "x, y = zip(*top)\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x = x,\n",
    "    y = y\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Common stopwords in tweets with class 0',\n",
    "    template='plotly_dark'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list_1 = words_list(1)\n",
    "dic = defaultdict(int)\n",
    "\n",
    "for word in words_list_0:\n",
    "    if word in stop:\n",
    "        dic[word] += 1\n",
    "        \n",
    "top=sorted(dic.items(), key=lambda x:x[1], reverse=True)[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "x, y = zip(*top)\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x = x,\n",
    "    y = y\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Common stopwords in tweets with class 1',\n",
    "    template='plotly_dark'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Analyzing punctuations</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = words_list(0)\n",
    "\n",
    "dic = defaultdict(int)\n",
    "special = string.punctuation\n",
    "for i in punctuations:\n",
    "    if i in special:\n",
    "        dic[i] += 1\n",
    "\n",
    "x, y = zip(*dic.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x = x,\n",
    "    y = y\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Punctuations for class 0',\n",
    "    template='plotly_dark'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = words_list(1)\n",
    "\n",
    "dic = defaultdict(int)\n",
    "special = string.punctuation\n",
    "for i in punctuations:\n",
    "    if i in special:\n",
    "        dic[i] += 1\n",
    "\n",
    "x, y = zip(*dic.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x = x,\n",
    "    y = y\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Punctuations for class 1',\n",
    "    template='plotly_dark'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Common words</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(words_list(1))\n",
    "most = counter.most_common()\n",
    "x = list()\n",
    "y = list()\n",
    "\n",
    "for word, count in most[:40]:\n",
    "    if word not in stop:\n",
    "        x.append(word)\n",
    "        y.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x = x,\n",
    "    y = y\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Common words for class 1',\n",
    "    template='plotly_dark'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(words_list(0))\n",
    "most = counter.most_common()\n",
    "x = list()\n",
    "y = list()\n",
    "\n",
    "for word, count in most[:40]:\n",
    "    if word not in stop:\n",
    "        x.append(word)\n",
    "        y.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x = x,\n",
    "    y = y\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Common words for class 0',\n",
    "    template='plotly_dark'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ngram analysis</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tweet_bigrams(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2,2)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "top_tweet_bigrams = get_top_tweet_bigrams(tweet['text'])[:10]\n",
    "x, y = map(list, zip(*top_tweet_bigrams))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x = x,\n",
    "    y = y\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text = 'Bigrams',\n",
    "    template='plotly_dark'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Cleaning </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([tweet, test])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Removing urls </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"New competition launched :https://www.kaggle.com/c/nlp-getting-started\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_URL(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete url from all rows\n",
    "df['text'] = df['text'].apply(lambda x: remove_URL(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Removing HTML tags</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"<div>\n",
    "<h1>Real or Fake</h1>\n",
    "<p>Kaggle </p>\n",
    "<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n",
    "</div>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(remove_html(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete HTML from all rows\n",
    "df['text'] = df['text'].apply(lambda x: remove_html(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Removing Emojis</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "remove_emoji(\"Omg another Earthquake 😔😔\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete emojis from all rows\n",
    "df['text'] = df['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Removing punctuations</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    \"\"\"This function creates a dictionary mapping of every character from string.punctuation to None\"\"\"\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"I am a #king\"\n",
    "print(remove_punctuations(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete punctuations from all rows\n",
    "df['text'] = df['text'].apply(lambda x: remove_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Vectorization </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = df[:7613]\n",
    "data_test = df[7613:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_train['text']\n",
    "y = data_train['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(X_train) # Vectors\n",
    "val_vectors = vectorizer.transform(X_val)\n",
    "test_vectors = vectorizer.transform(data_test['text'])\n",
    "\n",
    "print(train_vectors.shape, val_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert vectorized and splited data into one dataframe to make evaluation simpler\n",
    "to_model_evaluation = pd.concat([pd.DataFrame(train_vectors.toarray()), pd.DataFrame(val_vectors.toarray())])\n",
    "y_all = pd.concat([y_train, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(model):\n",
    "    # Train our model\n",
    "    model.fit(train_vectors, y_train)\n",
    "    y_pred = model.predict(val_vectors)\n",
    "\n",
    "    # F1 score\n",
    "    scores_f1 = cross_val_score(model, to_model_evaluation, y_all, scoring = 'f1' , cv=4)\n",
    "    print('F1 scores: {}'.format(scores_f1))\n",
    "    print('F1 mean score: {}'.format(scores_f1.mean()))\n",
    "\n",
    "    # Accuracy score\n",
    "    scores_acc = cross_val_score(model, to_model_evaluation, y_all, cv=4)\n",
    "    print('Acc scores: {}'.format(scores_acc))\n",
    "    print('Acc mean score: {}'.format(scores_acc.mean()))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print('Confusion Matrix: ')\n",
    "    matrix = confusion_matrix(y_val, y_pred)\n",
    "    group_names = ['True Negative','False Positive','False Negative','True Positive']\n",
    "    group_counts =['{0:0.0f}'.format(value) for value in matrix.flatten()]\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in matrix.flatten()/np.sum(matrix)]\n",
    "    \n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2, 2)\n",
    "\n",
    "    sns.heatmap(matrix, annot=labels, fmt='', cmap='rocket_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model Creation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classification\n",
    "NB_model = MultinomialNB()\n",
    "\n",
    "# LogisticRegression\n",
    "LR_model = LogisticRegression()\n",
    "\n",
    "# LGBMClassifier\n",
    "LGM_model = LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(NB_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(LR_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(LGM_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv('D:/Python/Anaconda/Kaggle/Working/Natural Language Processing with Disaster Tweets/nlp-getting-started/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre=LR_model.predict(test_vectors)\n",
    "y_pre=np.round(y_pre).astype(int).reshape(3263)\n",
    "sub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\n",
    "sub.to_csv('submission2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
